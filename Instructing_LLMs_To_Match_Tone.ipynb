{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/EasyGPT-2023/blob/main/Instructing_LLMs_To_Match_Tone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d39d60",
      "metadata": {
        "id": "72d39d60"
      },
      "source": [
        "# Instructing LLMs To Match Tone\n",
        "\n",
        "LLMs that generate text are awesome, but what if you want to edit the tone/style it responds with?\n",
        "\n",
        "We've all seen the [pirate](https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_agent.html#:~:text=template%20%3D%20%22%22%22Answer%20the%20following%20questions%20as%20best%20you%20can%2C%20but%20speaking%20as%20a%20pirate%20might%20speak.%20You%20have%20access%20to%20the%20following%20tools%3A) examples, but it would be awesome if we could tune the prompt to match the tone of specific people?\n",
        "\n",
        "Below is a series of techniques aimed to generate text in the tone and style you want. No single technique will likely be *exactly* what you need, but I guarantee you can iterate with these tips to get a solid outcome for your project.\n",
        "\n",
        "But Greg, what about fine tuning? Fine tuning would likely give you a fabulous result, but the barriers to entry are too high for the average developer (as of May '23). I would rather get the 87% solution today rather than not ship something. If you're doing this in production and your differentiator is your ability to adapt to different styles you'll likely want to explore fine tuning.\n",
        "\n",
        "If you want to see a demo video of this, check out the Twitter post. For a full explination head over to YouTube.\n",
        "\n",
        "### 4 Levels Of Tone Matching Techniques:\n",
        "1. **Simple:** As a human, try and describe the tone you would like\n",
        "2. **Intermediate:** Include your description + examples\n",
        "3. **AI-Assisted:** Ask the LLM to extract tone, use their output in your next prompt\n",
        "4. **Technique Fusion:** Combine multiple techniques to mimic tone\n",
        "\n",
        "**Today's Goal**: Generate tweets mimicking the style of online personalities. You could customize this code to generate emails, chat messages, writing, etc.\n",
        "\n",
        "First let's import our packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "b7dF6JTihsK0"
      },
      "id": "b7dF6JTihsK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SiEzUvdrhz80"
      },
      "id": "SiEzUvdrhz80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e65bd69a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e65bd69a",
        "outputId": "1647252d-3465-45f9-f044-1e5fccd42f5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# LangChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Environment Variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Twitter\n",
        "import tweepy\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83886158",
      "metadata": {
        "id": "83886158"
      },
      "source": [
        "Set your OpenAI key. You can either put it as an environment variable or in the string below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "98123655",
      "metadata": {
        "hide_input": false,
        "id": "98123655"
      },
      "outputs": [],
      "source": [
        "openai_api_key = os.getenv('OPENAI_API_KEY', 'sk-')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e572c7",
      "metadata": {
        "id": "60e572c7"
      },
      "source": [
        "We'll be using `gpt-4` today, but you can swap out for `gpt-3.5-turbo` if you'd like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "063daa43",
      "metadata": {
        "id": "063daa43"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key, model_name='gpt-4')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76a9b7c",
      "metadata": {
        "id": "c76a9b7c"
      },
      "source": [
        "## Method #1: Simple - Describe the tone you would like\n",
        "\n",
        "Our first method is going to be simply describing the tone we would like.\n",
        "\n",
        "Let's try a few exmaples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0c852071",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c852071",
        "outputId": "f01d1ee8-8309-482f-a6ba-a3692bca1ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"ê³ ê°ë‹˜ì˜ ë§Œì¡±ì´ ì €í¬ì—ê²Œ í° í˜ì´ ë©ë‹ˆë‹¤! ì•ìœ¼ë¡œë„ ì¢‹ì€ ìƒí’ˆìœ¼ë¡œ ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤ ğŸ˜ŠğŸ™ğŸ‘\"\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "'ë³„ ê¸°ëŒ€ ì—†ì´ ìƒ€ëŠ”ë° ìƒë‹¹íˆ ì˜·ì´ ê´œì°®ê³  ì˜ˆì˜ë„¤ìš” ê¹”ë³„ë¡œ ìŸì¼ ìƒê° ë¬´íƒ ë‹¤ë“œ ê°€ì„±ë¹„ ì˜¤ì§€ë„¤ìš”' ì— ëŒ€í•œ ëŒ“ê¸€ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\n",
        "\n",
        "% RESPONSE CONTENT:\n",
        "    - Write a comment with gratitude for using the shopping mall and a desire for continued business.\n",
        "\n",
        "% RESPONSE FORMAT:\n",
        "\n",
        "    - Respond in under 500 characters\n",
        "    - Respond in three or less short sentences\n",
        "    - emojis\n",
        "\"\"\"\n",
        "\n",
        "output = llm.predict(prompt)\n",
        "print (output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}